{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huggingface Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8409e874658412c8ed07c2887b4b62a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "model = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = pipeline(\n",
    "    'I liked \"Breaking Bad\" and \"Band of Brothers\". Do you have any recommendations of other shows I might like?\\n',\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_length=200,\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = pipeline(\n",
    "    'I liked \"Breaking Bad\" and \"Band of Brothers\". Do you have any recommendations of other shows I might like?\\n',\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_length=200,\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input = \"a power cord on a ski lift\"\n",
    "#see how the tokenizer tokenizes the input\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "print(tokenizer.tokenize(sample_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Probabilities of generated tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9788af9952f2407bacc0d5ecd0459a7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, LlamaForCausalLM\n",
    "\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "model = LlamaForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "def give_prompts_outputs_and_lengths(desc_file_name):\n",
    "\n",
    "    input_path_prompt = \"/export/home/ru86qer/classify_by_description_release/prompts/prompt_0.txt\"\n",
    "\n",
    "    with open(input_path_prompt, 'r') as f:\n",
    "        prompt = f.read()\n",
    "\n",
    "    file_name_1 = desc_file_name.split(\".\")[0] + \"_whole_outputs.json\"\n",
    "    input_path_whole_outputs = '/export/home/ru86qer/classify_by_description_release/descriptors_meta_info/'+file_name_1\n",
    "\n",
    "    with open(input_path_whole_outputs, 'r') as f:\n",
    "        whole_outputs = json.load(f)\n",
    "\n",
    "    classes = list(whole_outputs.keys())\n",
    "\n",
    "    lengths = []\n",
    "    prompts_plus_outputs = [prompt]*len(classes)\n",
    "    for i,class_name in enumerate(classes):\n",
    "        prompt_plus_output = prompts_plus_outputs[i].format(category_name=class_name.replace('_', ' '))\n",
    "        lengths.append(len(prompt_plus_output))\n",
    "        prompts_plus_outputs[i] = prompt_plus_output\n",
    "        prompts_plus_outputs[i] += whole_outputs[class_name]\n",
    "    \n",
    "    return prompts_plus_outputs, lengths\n",
    "\n",
    "\n",
    "# desc_file_name = \"descriptors_cub_llama2_prompt_0_doSampleFalse_run_0.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_plus_outputs_1, lengths_1 =give_prompts_outputs_and_lengths(\"descriptors_cub_llama2_prompt_0_run_0.json\")\n",
    "prompts_plus_outputs_2, lengths_2 =give_prompts_outputs_and_lengths(\"descriptors_cub_llama2_prompt_0_topk1_run_0.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tqdm\n",
    "\n",
    "def give_token_probs(tokenizer, model, prompts_plus_outputs, lengths):\n",
    "    \n",
    "    acc = []\n",
    "\n",
    "    for i, input_text in tqdm.tqdm(enumerate(prompts_plus_outputs)):\n",
    "        \n",
    "        input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids)\n",
    "            logits = outputs.logits\n",
    "\n",
    "        \n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "        generated_answer = input_text[lengths[i]:]\n",
    "        gen_answer_ids = tokenizer.encode(generated_answer)\n",
    "        gen_answer_probs = [probs[0, i, input_ids[0, i]].item() for i in range(len(input_ids[0]) - len(gen_answer_ids), len(input_ids[0]))]\n",
    "\n",
    "        \n",
    "        average_probability = sum(gen_answer_probs) / len(gen_answer_probs)\n",
    "\n",
    "        acc.append(average_probability)\n",
    "\n",
    "    print(\"Average of averages {0}\".format(sum(acc) / len(acc)))\n",
    "    \n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [25:20,  7.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average of averages 0.006796153096871086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [16:01,  4.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average of averages 0.010578365354993135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "acc_topk10 = give_token_probs(tokenizer, model,prompts_plus_outputs_1,lengths_1)\n",
    "acc_topk1 = give_token_probs(tokenizer, model,prompts_plus_outputs_2,lengths_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min acc_top1:  6.755461273571999e-05\n",
      "index of min acc_top1:  123\n"
     ]
    }
   ],
   "source": [
    "print(\"min acc_top1: \", min(acc_topk1))\n",
    "print(\"index of min acc_top1: \", acc_topk1.index(min(acc_topk1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max acc_top1:  0.02975448280754543\n",
      "index of max acc_top1:  59\n"
     ]
    }
   ],
   "source": [
    "print(\"max acc_top1: \", max(acc_topk1))\n",
    "print(\"index of max acc_top1: \", acc_topk1.index(max(acc_topk1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimal average token probability: \n",
      "Q: What are useful visual features for distinguishing a lemur in a photo?\n",
      "A: There are several useful visual features to tell there is a lemur in a photo:\n",
      "- four-limbed primate\n",
      "- black, grey, white, brown, or red-brown\n",
      "- wet and hairless nose with curved nostrils\n",
      "- long tail\n",
      "- large eyes\n",
      "- furry bodies\n",
      "- clawed hands and feet\n",
      "\n",
      "Q: What are useful visual features for distinguishing a television in a photo?\n",
      "A: There are several useful visual features to tell there is a television in a photo:\n",
      "- electronic device\n",
      "- black or grey\n",
      "- a large, rectangular screen\n",
      "- a stand or mount to support the screen\n",
      "- one or more speakers\n",
      "- a power cord\n",
      "- input ports for connecting to other devices\n",
      "- a remote control\n",
      "\n",
      "Q: What are useful features for distinguishing a Le_Conte_Sparrow in a photo?\n",
      "A: There are several useful visual features to tell there is a Le_Conte_Sparrow in a photo:\n",
      "- small, dark sparrow\n",
      "- distinctive black stripes on the head, neck, and back\n",
      "- white stripes on the face and chest\n",
      "- dark wings with a distinctive white patch on the secondaries\n",
      "- long, dark\n",
      "\n",
      "\n",
      " ############################################################## \n",
      "\n",
      "\n",
      "Maximum average token probability: \n",
      "Q: What are useful visual features for distinguishing a lemur in a photo?\n",
      "A: There are several useful visual features to tell there is a lemur in a photo:\n",
      "- four-limbed primate\n",
      "- black, grey, white, brown, or red-brown\n",
      "- wet and hairless nose with curved nostrils\n",
      "- long tail\n",
      "- large eyes\n",
      "- furry bodies\n",
      "- clawed hands and feet\n",
      "\n",
      "Q: What are useful visual features for distinguishing a television in a photo?\n",
      "A: There are several useful visual features to tell there is a television in a photo:\n",
      "- electronic device\n",
      "- black or grey\n",
      "- a large, rectangular screen\n",
      "- a stand or mount to support the screen\n",
      "- one or more speakers\n",
      "- a power cord\n",
      "- input ports for connecting to other devices\n",
      "- a remote control\n",
      "\n",
      "Q: What are useful features for distinguishing a Glaucous_winged_Gull in a photo?\n",
      "A: There are several useful visual features to tell there is a Glaucous_winged_Gull in a photo:\n",
      "- large, white wing patches\n",
      "- dark grey back and wings\n",
      "- pinkish-gray legs and feet\n",
      "- black bill\n",
      "- pale yellow head and neck\n",
      "- dark eye patch\n",
      "- distinctive wing shape\n",
      "\n",
      "Q: What\n"
     ]
    }
   ],
   "source": [
    "print(\"Minimal average token probability: \\n\"+prompts_plus_outputs_2[123])\n",
    "print(\"\\n\\n ############################################################## \\n\\n\")\n",
    "print(\"Maximum average token probability: \\n\"+prompts_plus_outputs_2[59])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min acc_top5:  4.603858009410944e-05\n",
      "index of min acc_top5:  177\n"
     ]
    }
   ],
   "source": [
    "print(\"min acc_top5: \", min(acc_topk10))\n",
    "print(\"index of min acc_top5: \", acc_topk10.index(min(acc_topk10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: Please answer the following question using bullet points '-'. Behind each bullet point, a sentence shall be in a format that is shown in the following template:\n",
      "\n",
      "- While for A feature X looks like this, for B feature X looks like that.\n",
      "\n",
      "Ideally, feature X is a visual feature that is useful for distinguishing a Black footed albatross from a Laysan albatross in a photo.\n",
      "\n",
      "It is essential to use the keyword 'While' and the comma (',') because we need it for further processing. Please do not use any other commas in your sentences.\n",
      "\n",
      "When you have answered the question, please just stop the output.\n",
      "\n",
      "Now comes your part:\n",
      "Q: What are useful features for distinguishing a Black footed albatross from a Laysan albatross in a photo?\n",
      "A: There are several useful visual features to tell apart a Black footed albatross from a a Laysan albatross in a photo:\n",
      "- While  the Black footed albatross has a distinctive white stripe on its upperpart, the Laysan albatross has a more mottled appearance with less distinct stripes.\n",
      "- While the Black footed albatross has a darker bill with a distinctive yellow spot, the Laysan albatross has a lighter bill with a more rounded tip.\n",
      "- While the Black footed albatross has a more slender neck and legs, the Laysan albatross has a thicker neck and legs.\n",
      "- While the Black footed albatross has a more rounded tail, the Laysan albatross has a more wedge-shaped tail.\n",
      "- While the Black footed albatross has a more mottled appearance overall, the Laysan albatross has a more uniform gray-brown coloration.\n",
      "\n",
      "Please answer the question using the bullet points and the format provided.\n"
     ]
    }
   ],
   "source": [
    "path = \"/export/home/ru86qer/classify_by_description_release/prompts/prompt_1.txt\"\n",
    "\n",
    "with open(path, 'r') as f:\n",
    "    prompt = f.read()\n",
    "\n",
    "sequences = pipeline(\n",
    "    prompt.format(category_name_1=\"Black footed albatross\", category_name_0=\"Laysan albatross\"),\n",
    "    do_sample=True,\n",
    "    top_k=1,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_length=500,\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate contrastive responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_to_contrast =[(\"Red cockaded Woodpecker\", \"American Three toed Woodpecker\"),\n",
    "                    (\"Louisiana Waterthrush\", \"Northern Waterthrush\"),\n",
    "                    (\"Chuck will Widow\",\"Nighthawk\"),\n",
    "                    (\"Anna Hummingbird\",\"Ruby throated Hummingbird\"),\n",
    "                    (\"Artic Tern\",\"Common Tern\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name= \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(\"Cuda available: \", torch.cuda.is_available())\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map = \"auto\",\n",
    ")\n",
    "\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "inputs = inputs.to('cuda')\n",
    "# Generate\n",
    "generate_ids = model.generate(inputs.input_ids, max_length=len(inputs.input_ids[0])+answer_length, top_k=top_k, num_return_sequences=1, do_sample=True)\n",
    "\n",
    "decoded_responses = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "response_texts = [resp[len(prompt):] for resp in decoded_responses.split('\\n-')]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "class",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
